{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit\n",
    "from qiskit.tools.visualization import plot_histogram\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn \n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qiskit Environment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuitEnvironment():\n",
    "    \n",
    "    def __init__(self, n_qbits, goal_state, max_t, graphics=False):\n",
    "        self.n_qbits = n_qbits\n",
    "        self.goal_state = goal_state # Used to calculate reward on each environment step\n",
    "        assert(len(goal_state) == (2 ** self.n_qbits)), \"goal_state must match the outcomes possible with n_qbits\"\n",
    "        self.graphics = graphics\n",
    "        \"\"\"\n",
    "        self.n_shots needs to be an arbirarily large number otherwise the variance in intermediate and final\n",
    "        states will be so large that the model will never converge.  Also it would cause some episodes to never \n",
    "        end due to the terminal criteria of the environment.\n",
    "        \"\"\"\n",
    "        self.max_t = max_t\n",
    "        self.t = 0\n",
    "        self.n_shots = 100000 \n",
    "        self.sim = qiskit.Aer.get_backend('qasm_simulator')\n",
    "        self.qc = qiskit.QuantumCircuit(self.n_qbits, self.n_qbits)\n",
    "        self.actions = {0:(lambda:self.qc.h(0)),1:(lambda:self.qc.h(1)),2:(lambda:self.qc.h(2)), \n",
    "                        3:(lambda:self.qc.x(0)),4:(lambda:self.qc.x(1)),5:(lambda:self.qc.x(2)), \n",
    "                        6:(lambda:self.qc.cx(0,1)),7:(lambda:self.qc.cx(0,2)),8:(lambda:self.qc.cx(1,2)),\n",
    "                        9:(lambda:self.qc.cx(1,0)),10:(lambda:self.qc.cx(2,0)),11:(lambda:self.qc.cx(2,1))}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.t += 1\n",
    "        # Delete the 3 measurements of the previous env step, not possible with an actual quantum register.\n",
    "        self.qc.data = self.qc.data[:-3] \n",
    "        self.actions[action]()\n",
    "        if self.graphics:\n",
    "            display(self.qc.draw(output='mpl'))\n",
    "        self.qc.measure([i for i in range(self.n_qbits)], [i for i in range(self.n_qbits)])\n",
    "        counts = qiskit.execute(self.qc, backend=self.sim, shots=self.n_shots).result().get_counts()     \n",
    "        max_diff = 0\n",
    "        next_state = []\n",
    "        for key in list(goal_state.keys()):\n",
    "            next_state.append(counts.get(key, 0) / self.n_shots)\n",
    "            diff = abs(next_state[-1] - goal_state[key])\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "        done = True if max_diff < 0.02 or self.t == self.max_t else False\n",
    "#         if done and self.graphics:\n",
    "#             display(plot_histogram(counts))  \n",
    "        reward = -1 if not done else 0\n",
    "        next_state = np.array(next_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.qc.data = []\n",
    "        if self.graphics:\n",
    "            display(self.qc.draw(output='mpl'))\n",
    "        self.t = 0\n",
    "        # Qiskit always starts all qbits in 0 state, so starting state is 100% all qbits '000'\n",
    "        start_state = np.array([1.0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        return start_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self, device):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(ActionValueNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def save_checkpoint(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_checkpoint(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=1, buffer_size=10000,\n",
    "                 batch_size=128, gamma=0.99, tau=.001, alpha=.001, update_every=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.update_every = update_every\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = ActionValueNetwork(state_size, action_size, seed).to(self.device)\n",
    "        self.qnetwork_target = ActionValueNetwork(state_size, action_size, seed).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=alpha)\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample(self.device)\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Update target networks if necessary\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        self.qnetwork_local.save_checkpoint(filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        self.qnetwork_local.load_checkpoint(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Goal State Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal State Specification\n",
    "n_qbits = 3\n",
    "goal_state = {'000' : 0, '001' : 0.25, '010' : 0.25, '011' : 0, '100' : 0, '101' : 0.25, '110' : 0.25, '111' : 0}\n",
    "max_t = 10 # Max timesteps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch Trained Agent Create to meet goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 6\n",
      "Next State:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 261.177x264.88 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal State Reached!\n",
      "Score: -3\n"
     ]
    }
   ],
   "source": [
    "env = QuantumCircuitEnvironment(n_qbits, goal_state, max_t, graphics=True)\n",
    "action_size = len(env.actions)\n",
    "state_size = len(env.goal_state)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "agent.load_model('best_model.pt')\n",
    "for episode in range(testing_episodes):\n",
    "    clear_output(wait = True)\n",
    "    print('Goal State:', list(goal_state.values()))\n",
    "    print('Start State:')\n",
    "    s = env.reset()\n",
    "    time.sleep(15)\n",
    "    score = 0\n",
    "    d = False\n",
    "    while not d:\n",
    "        a = agent.act(s, 0)\n",
    "        clear_output(wait=True)\n",
    "        print('Action:', a)\n",
    "        print('Next State:')\n",
    "        s_, r, d = env.step(a)\n",
    "        time.sleep(3)\n",
    "        score += r\n",
    "        s = s_\n",
    "    print('Goal State Reached!')\n",
    "    print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
